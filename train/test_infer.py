import torch
from transformers import AutoTokenizer
from model import ModeClassifier  # âœ… ë¶„ë¥˜ ì „ìš© ëª¨ë¸ë¡œ ë³€ê²½

def predict(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=64)
    input_ids = inputs["input_ids"].cuda()
    attention_mask = inputs["attention_mask"].cuda()

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        class_logits = outputs["logits"]
    
    pred_class = torch.argmax(class_logits, dim=1).item()
    mode = "ëŒ€í™”" if pred_class == 0 else "ì§€ì‹œ"
    return mode

# ğŸ”½ í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ
if __name__ == "__main__":
    model_name = "klue/bert-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    model = ModeClassifier(model_name)
    model.load_state_dict(torch.load("saved_models/mode_classifier.pt"))
    model.cuda()

    while True:
        text = input("ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” (exit ì…ë ¥ ì‹œ ì¢…ë£Œ): ")
        if text.lower() == "exit":
            break

        mode = predict(text, model, tokenizer)
        print(f"ğŸ§  ë¶„ë¥˜ ê²°ê³¼: {mode}")
